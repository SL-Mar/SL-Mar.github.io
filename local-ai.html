<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <meta name="description" content="Local AI Solutions by SL Mar - Self-Hosted AI Automation">
    <title>Local AI | SL Mar</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav>
        <div class="container">
            <a href="index.html" class="logo">SL Mar - Maritime and AI Consultancy</a>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="publications.html">Publications</a></li>
                <li><a href="development.html">Development</a></li>
                <li><a href="local-ai.html" class="active">Local AI</a></li>
                <li><a href="rnd-notes.html">R&D Notes</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </div>
    </nav>

    <section class="hero">
        <div class="container">
            <h1>Local AI</h1>
            <p>
                Self-hosted AI automation pipelines and local language models for privacy, cost reduction, and enhanced performance.
            </p>
        </div>
    </section>

    <section>
        <div class="container">
            <article class="pub">
                <h3><a href="https://medium.com/ai-advances/how-to-build-a-self-hosted-ai-automation-pipeline-with-docker-n8n-ollama-and-telegram-65a411a8f161" target="_blank">How to Build a Self-Hosted AI Automation Pipeline with Docker, n8n, Ollama, and Telegram</a></h3>
                <div class="pub-meta">AI Advances • Sep 15, 2025 • 13 min read</div>
                <p>
                    Complete guide to building a self-hosted AI automation pipeline using Docker, n8n workflows, Ollama for local LLMs, and Telegram for notifications. This comprehensive tutorial walks through setting up a complete infrastructure for running AI automation entirely on your own hardware, ensuring data privacy and eliminating API costs.
                </p>
            </article>

            <article class="pub">
                <h3><a href="https://medium.com/ai-advances/openai-on-your-desk-why-moving-to-local-models-ed81fb993aab" target="_blank">OpenAI on your desk: Why Moving to Local Models</a></h3>
                <div class="pub-meta">AI Advances • Aug 8, 2025 • 7 min read</div>
                <p>
                    How to Shift Projects in a Rapidly Evolving Environment - exploring the benefits and practical considerations of running large language models locally. This article discusses the strategic decision to move from cloud-based AI services to self-hosted solutions, covering performance comparisons, cost analysis, and implementation strategies for local AI deployment.
                </p>
            </article>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 SL Mar</p>
            <p>AI for Quantitative Finance and Maritime Applications</p>
            <p>France • SIRET 98770400400017</p>
            <p><a href="MC_mentions-legales.html" style="color: inherit; text-decoration: underline;">Mentions légales</a></p>
        </div>
    </footer>
</body>
</html>
