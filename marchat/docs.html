<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MarChat - Pre-Arrival Form Automation</title>
    <link rel="stylesheet" href="assets/css/docs.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>

    <!-- Header -->
    <header class="docs-header">
        <div class="docs-header-content">
            <a href="docs.html" class="docs-logo">
                <i class="fas fa-ship"></i>
                <span>MarChat</span>
            </a>
            <nav>
                <ul class="docs-nav">
                    <li><a href="docs.html" class="active">Documentation</a></li>
                    <li><a href="https://github.com/SL-Mar/marchat" target="_blank"><i class="fab fa-github"></i> GitHub</a></li>
                </ul>
            </nav>
            <a href="../index.html" class="back-to-main"><i class="fas fa-arrow-left"></i> Back to Main Site</a>
        </div>
    </header>

    <!-- Documentation Layout -->
    <div class="docs-container">

        <!-- Sidebar -->
        <aside class="docs-sidebar">

            <div class="sidebar-section">
                <div class="sidebar-title">Product</div>
                <ul class="sidebar-menu">
                    <li><a href="#overview"><i class="fas fa-home"></i> Overview</a></li>
                    <li><a href="#how-it-works"><i class="fas fa-cogs"></i> How It Works</a></li>
                    <li><a href="#screenshots"><i class="fas fa-images"></i> Screenshots</a></li>
                </ul>
            </div>

            <div class="sidebar-section">
                <div class="sidebar-title">Technical</div>
                <ul class="sidebar-menu">
                    <li><a href="#architecture"><i class="fas fa-sitemap"></i> Architecture</a></li>
                    <li><a href="#llm-providers"><i class="fas fa-brain"></i> LLM Providers</a></li>
                    <li><a href="#api-reference"><i class="fas fa-plug"></i> API Reference</a></li>
                </ul>
            </div>

            <div class="sidebar-section">
                <div class="sidebar-title">Deployment</div>
                <ul class="sidebar-menu">
                    <li><a href="#installation"><i class="fas fa-download"></i> Installation</a></li>
                    <li><a href="#configuration"><i class="fas fa-sliders-h"></i> Configuration</a></li>
                </ul>
            </div>

            <div class="sidebar-section">
                <div class="sidebar-title">Resources</div>
                <ul class="sidebar-menu">
                    <li><a href="https://github.com/SL-Mar/marchat" target="_blank"><i class="fab fa-github"></i> GitHub</a></li>
                </ul>
            </div>

        </aside>

        <!-- Main Content -->
        <main class="docs-main">
            <div class="docs-content">

                <!-- ==================== OVERVIEW ==================== -->
                <section id="overview">
                    <h1>MarChat</h1>
                    <p class="docs-subtitle">AI-powered pre-arrival form automation for maritime operations</p>

                    <div class="alert info">
                        <strong><i class="fas fa-check-circle"></i> Commercial Product</strong><br>
                        MarChat is a delivered product built for a maritime operator. It automates the filling of pre-arrival port documentation using a tool-calling LLM agent with local-first inference and cloud fallback. Source code is available under Apache 2.0.
                    </div>

                    <p>
                        Pre-arrival documentation &mdash; FAL forms, ISPS declarations, health declarations, customs manifests &mdash; is a recurring burden for every port call. MarChat eliminates manual data entry by reading source documents (crew lists, stores declarations, certificates) and using an AI agent to fill Excel, DOCX, and PDF templates automatically.
                    </p>
                    <p>
                        The system runs on commodity hardware with no cloud dependency. All processing stays on the shipboard network via Ollama. An optional Anthropic Claude fallback provides resilience when local inference is unavailable.
                    </p>

                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4><i class="fas fa-robot"></i> Tool-Calling Agent</h4>
                            <ul>
                                <li>Multi-turn agent loop (up to 5 iterations)</li>
                                <li>5 tools: vessel profiles, source docs, PDF extraction, field schema, submit values</li>
                                <li>Automatic fallback to JSON prompt if tool-calling fails</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="fas fa-file-signature"></i> Template Engine</h4>
                            <ul>
                                <li>Excel (.xlsx), Word (.docx), PDF form templates</li>
                                <li><code>{{placeholder}}</code> field detection</li>
                                <li>Batch fill all templates with SSE progress streaming</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="fas fa-ship"></i> Ship's Knowledge Base</h4>
                            <ul>
                                <li>Upload crew lists, certificates, stores declarations</li>
                                <li>Vessel profiles with reusable ship particulars</li>
                                <li>Context injected automatically into agent prompts</li>
                            </ul>
                        </div>
                        <div class="feature-card">
                            <h4><i class="fas fa-shield-halved"></i> Local-First</h4>
                            <ul>
                                <li>All inference via Ollama &mdash; no data leaves the vessel</li>
                                <li>Optional Anthropic Claude fallback for resilience</li>
                                <li>Runs on standard shipboard hardware</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <!-- ==================== HOW IT WORKS ==================== -->
                <section id="how-it-works">
                    <h2>How It Works</h2>

                    <p>The auto-fill pipeline follows a straightforward workflow:</p>

                    <div class="formula-block">
                        <strong>Source Documents + Instructions + Vessel Profile</strong>
                        &rarr; <strong>Tool-Calling Agent</strong>
                        &rarr; <strong>Filled Templates</strong>
                    </div>

                    <ol style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                        <li><strong>Upload source documents</strong> &mdash; crew lists, stores declarations, certificates, any reference material</li>
                        <li><strong>Upload form templates</strong> &mdash; Excel/DOCX/PDF files with <code>{{placeholder}}</code> fields</li>
                        <li><strong>Provide instructions</strong> &mdash; voyage details, port of call, ETA, agent email context</li>
                        <li><strong>Agent fills forms</strong> &mdash; the LLM reads source docs via tools, maps data to template fields, calls <code>submit_field_values</code></li>
                        <li><strong>Download results</strong> &mdash; individually or as a batch .zip archive</li>
                    </ol>

                    <h3>Provider Fallback Chain</h3>
                    <p>In <code>auto</code> mode (default), the system tries providers in order until one succeeds:</p>
                    <table>
                        <thead>
                            <tr>
                                <th>Step</th>
                                <th>Provider</th>
                                <th>Method</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1</td>
                                <td>Ollama (qwen2.5:14b)</td>
                                <td>Native tool-calling</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>Anthropic (Claude)</td>
                                <td>Tool-calling with format conversion</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>Best available</td>
                                <td>Single-shot JSON prompt fallback</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <!-- ==================== SCREENSHOTS ==================== -->
                <section id="screenshots">
                    <h2>Screenshots</h2>

                    <p><strong>Pre-Arrival workspace &mdash; blank template with field placeholders:</strong></p>
                    <img src="assets/images/pre-arrival-blank.png" alt="MarChat Pre-Arrival blank template" style="width:100%; border:1px solid #334155; border-radius:8px; margin-bottom:1.5rem;">

                    <p><strong>Filled form after agent auto-fill with tool call log:</strong></p>
                    <img src="assets/images/pre-arrival-filled.png" alt="MarChat Pre-Arrival filled form with agent log" style="width:100%; border:1px solid #334155; border-radius:8px; margin-bottom:1.5rem;">
                </section>

                <!-- ==================== ARCHITECTURE ==================== -->
                <section id="architecture">
                    <h2>Architecture</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Component</th>
                                <th>Technology</th>
                                <th>Role</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Frontend</td>
                                <td>Next.js 14, React, Tailwind CSS</td>
                                <td>Single-page pre-arrival workspace</td>
                            </tr>
                            <tr>
                                <td>Backend</td>
                                <td>FastAPI, SQLAlchemy, Python</td>
                                <td>API, agent loop, template engine</td>
                            </tr>
                            <tr>
                                <td>Database</td>
                                <td>SQLite</td>
                                <td>Templates, profiles, filled forms</td>
                            </tr>
                            <tr>
                                <td>LLM (primary)</td>
                                <td>Ollama (qwen2.5:14b)</td>
                                <td>Tool-calling agent, form filling</td>
                            </tr>
                            <tr>
                                <td>LLM (fallback)</td>
                                <td>Anthropic (Claude)</td>
                                <td>Cloud fallback when Ollama unavailable</td>
                            </tr>
                            <tr>
                                <td>Documents</td>
                                <td>openpyxl, python-docx, pypdf</td>
                                <td>Template parsing and filling</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>Agent Tools</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Tool</th>
                                <th>Purpose</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><code>get_template_fields</code></td>
                                <td>Read field schema from a form template</td>
                            </tr>
                            <tr>
                                <td><code>get_vessel_profile</code></td>
                                <td>Retrieve stored vessel particulars</td>
                            </tr>
                            <tr>
                                <td><code>read_source_document</code></td>
                                <td>Read extracted text from a source document</td>
                            </tr>
                            <tr>
                                <td><code>extract_pdf_text</code></td>
                                <td>Extract text from a PDF file</td>
                            </tr>
                            <tr>
                                <td><code>submit_field_values</code></td>
                                <td>Submit completed field-value mapping (terminal)</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <!-- ==================== LLM PROVIDERS ==================== -->
                <section id="llm-providers">
                    <h2>LLM Providers</h2>

                    <p>MarChat supports two LLM providers with automatic format conversion between them:</p>

                    <h3>Ollama (Local)</h3>
                    <p>Default provider. Uses <code>qwen2.5:14b</code> for tool-calling and <code>gemma3:12b</code> for general chat. All data stays on-device.</p>

                    <h3>Anthropic (Cloud)</h3>
                    <p>Optional fallback. The <code>AnthropicProvider</code> converts Ollama-format tool schemas and messages to Anthropic's API format transparently. Responses are converted back to the same internal format, so the agent loop is provider-agnostic.</p>

                    <div class="alert info">
                        <strong><i class="fas fa-exchange-alt"></i> Format Conversion</strong><br>
                        Tool schemas: Ollama <code>{type: "function", function: {name, parameters}}</code> &harr; Anthropic <code>{name, input_schema}</code><br>
                        Messages: <code>role: "tool"</code> &harr; Anthropic <code>tool_result</code> content blocks with <code>tool_use_id</code> matching
                    </div>
                </section>

                <!-- ==================== API REFERENCE ==================== -->
                <section id="api-reference">
                    <h2>API Reference</h2>

                    <h3>Forms API</h3>

                    <div class="endpoint">
                        <span class="method method-post">POST</span>
                        <code>/api/forms/templates/upload</code>
                        <p>Upload an Excel/DOCX/PDF form template. Returns extracted fields.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-post">POST</span>
                        <code>/api/forms/fill/auto</code>
                        <p>Auto-fill a template using the tool-calling agent. Accepts template ID, context text, vessel profile ID, and source document IDs.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-post">POST</span>
                        <code>/api/forms/fill/batch</code>
                        <p>Batch auto-fill multiple templates. Returns SSE stream with real-time progress events.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-post">POST</span>
                        <code>/api/forms/sources/upload</code>
                        <p>Upload a source document (crew list, stores declaration, etc.) for agent context.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-get">GET</span>
                        <code>/api/forms/filled/{id}/download</code>
                        <p>Download a filled form.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-get">GET</span>
                        <code>/api/forms/filled/download-batch?ids=1,2,3</code>
                        <p>Download multiple filled forms as a .zip archive.</p>
                    </div>

                    <h3>Settings &amp; Health</h3>

                    <div class="endpoint">
                        <span class="method method-get">GET</span>
                        <code>/health</code>
                        <p>Returns Ollama and Anthropic status. Healthy if either provider is available.</p>
                    </div>

                    <div class="endpoint">
                        <span class="method method-get">GET</span>
                        <code>/api/settings</code>
                        <p>Current configuration: models, provider mode.</p>
                    </div>
                </section>

                <!-- ==================== INSTALLATION ==================== -->
                <section id="installation">
                    <h2>Installation</h2>

                    <h3>Docker Compose</h3>
<pre><code># Pull required Ollama models
ollama pull qwen2.5:14b

# Start services
docker compose up -d</code></pre>

                    <p>Frontend: <code>http://localhost:3005</code> &mdash; Backend: <code>http://localhost:8005</code></p>

                    <h3>Dev Mode</h3>
<pre><code>bash launch.sh</code></pre>
                </section>

                <!-- ==================== CONFIGURATION ==================== -->
                <section id="configuration">
                    <h2>Configuration</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Variable</th>
                                <th>Default</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><code>OLLAMA_BASE_URL</code></td>
                                <td><code>http://localhost:11434</code></td>
                                <td>Ollama API endpoint</td>
                            </tr>
                            <tr>
                                <td><code>TOOL_CALLING_MODEL</code></td>
                                <td><code>qwen2.5:14b</code></td>
                                <td>Model for the tool-calling agent</td>
                            </tr>
                            <tr>
                                <td><code>ANTHROPIC_API_KEY</code></td>
                                <td>(none)</td>
                                <td>Anthropic API key for cloud fallback</td>
                            </tr>
                            <tr>
                                <td><code>LLM_PROVIDER</code></td>
                                <td><code>auto</code></td>
                                <td><code>auto</code>, <code>ollama</code>, or <code>anthropic</code></td>
                            </tr>
                        </tbody>
                    </table>
                </section>

            </div>
        </main>
    </div>

</body>
</html>
